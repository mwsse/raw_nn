{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks from scratch in Python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases  = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradient on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases  = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinputs  = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify the original variable, \n",
    "        # let's make a copy of the variable first \n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1,1)\n",
    "            # Calculate Jacobian matrix of the output ...\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "\n",
    "            # Calculate sample-wise gradient and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common loss class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        return np.mean(sample_losses)\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-entropy loss (subclass to Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forwad pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "\n",
    "        # Probabilities for target values - \n",
    "        # only if categorical labels\n",
    "        # correct_confidences = 0  # Remove warning!\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "        \n",
    "        #else:\n",
    "        #    print(\"ERROR!\")\n",
    "        #    correct_confidences = 0\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample.\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector \n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation_Softmax_Loss_CategoricalCrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "\n",
    "    # Creates activation and loss funtion objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss       = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        # If labels are one-hot encoded, turn them into discrete values\n",
    "        if len(y_true.shape) == 2: \n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "            \n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer SGD (Stochastic Gradient Descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "\n",
    "    def __init__(self, learning_rate=1.0, decay=0.0, momentum=0.0):\n",
    "        self.learning_rate          = learning_rate \n",
    "        self.current_learning_rate  = learning_rate\n",
    "        self.decay                  = decay\n",
    "        self.iterations             = 0\n",
    "        self.momentum               = momentum\n",
    "    \n",
    "    # Call once before any parameters update\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1.0 / (1.0 + self.decay * self.iterations)) \n",
    "\n",
    "    def update_params(self, layer):\n",
    "        if self.momentum:\n",
    "\n",
    "            # If layer does not contain momentum arrays, create \n",
    "            # them filled with zeroes (weight and biases)\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums   = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Build weight updates with momentum - take previous \n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = \\\n",
    "                self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weights_momentums = weight_updates\n",
    "\n",
    "            # .. and build biases updates \n",
    "            bias_updates = \\\n",
    "                self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentum = bias_updates\n",
    "\n",
    "        else:   \n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates   = -self.current_learning_rate * layer.dbiases\n",
    "        \n",
    "        layer.weights += weight_updates\n",
    "        layer.biases  += bias_updates\n",
    "        \n",
    "    # Call once after any parameters updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 00000 | acc: 0.353 | loss: 1.099 | lr: 1.0\n",
      "epoch: 00100 | acc: 0.467 | loss: 1.079 | lr: 0.9099181073703367\n",
      "epoch: 00200 | acc: 0.450 | loss: 1.067 | lr: 0.8340283569641367\n",
      "epoch: 00300 | acc: 0.447 | loss: 1.065 | lr: 0.7698229407236336\n",
      "epoch: 00400 | acc: 0.437 | loss: 1.064 | lr: 0.7147962830593281\n",
      "epoch: 00500 | acc: 0.430 | loss: 1.062 | lr: 0.66711140760507\n",
      "epoch: 00600 | acc: 0.427 | loss: 1.061 | lr: 0.6253908692933083\n",
      "epoch: 00700 | acc: 0.433 | loss: 1.059 | lr: 0.5885815185403178\n",
      "epoch: 00800 | acc: 0.453 | loss: 1.056 | lr: 0.5558643690939411\n",
      "epoch: 00900 | acc: 0.467 | loss: 1.054 | lr: 0.526592943654555\n",
      "epoch: 01000 | acc: 0.467 | loss: 1.051 | lr: 0.5002501250625312\n",
      "epoch: 01100 | acc: 0.463 | loss: 1.048 | lr: 0.4764173415912339\n",
      "epoch: 01200 | acc: 0.463 | loss: 1.045 | lr: 0.45475216007276037\n",
      "epoch: 01300 | acc: 0.467 | loss: 1.041 | lr: 0.43497172683775553\n",
      "epoch: 01400 | acc: 0.470 | loss: 1.037 | lr: 0.4168403501458941\n",
      "epoch: 01500 | acc: 0.480 | loss: 1.032 | lr: 0.4001600640256102\n",
      "epoch: 01600 | acc: 0.480 | loss: 1.026 | lr: 0.3847633705271258\n",
      "epoch: 01700 | acc: 0.490 | loss: 1.020 | lr: 0.3705075954057058\n",
      "epoch: 01800 | acc: 0.493 | loss: 1.014 | lr: 0.35727045373347627\n",
      "epoch: 01900 | acc: 0.493 | loss: 1.006 | lr: 0.3449465332873405\n",
      "epoch: 02000 | acc: 0.507 | loss: 0.998 | lr: 0.33344448149383127\n",
      "epoch: 02100 | acc: 0.510 | loss: 0.989 | lr: 0.32268473701193934\n",
      "epoch: 02200 | acc: 0.527 | loss: 0.979 | lr: 0.31259768677711786\n",
      "epoch: 02300 | acc: 0.530 | loss: 0.971 | lr: 0.3031221582297666\n",
      "epoch: 02400 | acc: 0.433 | loss: 0.976 | lr: 0.29420417769932333\n",
      "epoch: 02500 | acc: 0.433 | loss: 0.973 | lr: 0.2857959416976279\n",
      "epoch: 02600 | acc: 0.427 | loss: 0.968 | lr: 0.2778549597110308\n",
      "epoch: 02700 | acc: 0.430 | loss: 0.963 | lr: 0.2703433360367667\n",
      "epoch: 02800 | acc: 0.433 | loss: 0.959 | lr: 0.26322716504343247\n",
      "epoch: 02900 | acc: 0.440 | loss: 0.953 | lr: 0.25647601949217746\n",
      "epoch: 03000 | acc: 0.443 | loss: 0.948 | lr: 0.25006251562890724\n",
      "epoch: 03100 | acc: 0.443 | loss: 0.943 | lr: 0.2439619419370578\n",
      "epoch: 03200 | acc: 0.450 | loss: 0.939 | lr: 0.23815194093831865\n",
      "epoch: 03300 | acc: 0.460 | loss: 0.936 | lr: 0.23261223540358225\n",
      "epoch: 03400 | acc: 0.470 | loss: 0.930 | lr: 0.22732439190725165\n",
      "epoch: 03500 | acc: 0.480 | loss: 0.927 | lr: 0.22227161591464767\n",
      "epoch: 03600 | acc: 0.480 | loss: 0.923 | lr: 0.21743857360295715\n",
      "epoch: 03700 | acc: 0.477 | loss: 0.919 | lr: 0.21281123643328367\n",
      "epoch: 03800 | acc: 0.483 | loss: 0.915 | lr: 0.20837674515524068\n",
      "epoch: 03900 | acc: 0.480 | loss: 0.911 | lr: 0.20412329046744235\n",
      "epoch: 04000 | acc: 0.490 | loss: 0.907 | lr: 0.2000400080016003\n",
      "epoch: 04100 | acc: 0.503 | loss: 0.904 | lr: 0.19611688566385566\n",
      "epoch: 04200 | acc: 0.513 | loss: 0.900 | lr: 0.19234468166955185\n",
      "epoch: 04300 | acc: 0.520 | loss: 0.896 | lr: 0.18871485185884126\n",
      "epoch: 04400 | acc: 0.527 | loss: 0.892 | lr: 0.18521948508983144\n",
      "epoch: 04500 | acc: 0.533 | loss: 0.888 | lr: 0.18185124568103292\n",
      "epoch: 04600 | acc: 0.537 | loss: 0.885 | lr: 0.1786033220217896\n",
      "epoch: 04700 | acc: 0.537 | loss: 0.882 | lr: 0.1754693805930865\n",
      "epoch: 04800 | acc: 0.537 | loss: 0.879 | lr: 0.17244352474564578\n",
      "epoch: 04900 | acc: 0.537 | loss: 0.875 | lr: 0.16952025767079165\n",
      "epoch: 05000 | acc: 0.547 | loss: 0.871 | lr: 0.16669444907484582\n",
      "epoch: 05100 | acc: 0.547 | loss: 0.868 | lr: 0.16396130513198884\n",
      "epoch: 05200 | acc: 0.547 | loss: 0.865 | lr: 0.16131634134537828\n",
      "epoch: 05300 | acc: 0.553 | loss: 0.861 | lr: 0.15875535799333226\n",
      "epoch: 05400 | acc: 0.557 | loss: 0.857 | lr: 0.1562744178777934\n",
      "epoch: 05500 | acc: 0.563 | loss: 0.854 | lr: 0.15386982612709646\n",
      "epoch: 05600 | acc: 0.563 | loss: 0.849 | lr: 0.15153811183512653\n",
      "epoch: 05700 | acc: 0.563 | loss: 0.846 | lr: 0.14927601134497687\n",
      "epoch: 05800 | acc: 0.567 | loss: 0.843 | lr: 0.14708045300779526\n",
      "epoch: 05900 | acc: 0.567 | loss: 0.840 | lr: 0.14494854326714016\n",
      "epoch: 06000 | acc: 0.573 | loss: 0.836 | lr: 0.1428775539362766\n",
      "epoch: 06100 | acc: 0.583 | loss: 0.833 | lr: 0.1408649105507818\n",
      "epoch: 06200 | acc: 0.587 | loss: 0.830 | lr: 0.13890818169190167\n",
      "epoch: 06300 | acc: 0.600 | loss: 0.827 | lr: 0.13700506918755992\n",
      "epoch: 06400 | acc: 0.600 | loss: 0.824 | lr: 0.13515339910798757\n",
      "epoch: 06500 | acc: 0.600 | loss: 0.821 | lr: 0.13335111348179757\n",
      "epoch: 06600 | acc: 0.593 | loss: 0.817 | lr: 0.13159626266614027\n",
      "epoch: 06700 | acc: 0.597 | loss: 0.815 | lr: 0.12988699831146902\n",
      "epoch: 06800 | acc: 0.597 | loss: 0.812 | lr: 0.12822156686754713\n",
      "epoch: 06900 | acc: 0.597 | loss: 0.808 | lr: 0.126598303582732\n",
      "epoch: 07000 | acc: 0.603 | loss: 0.805 | lr: 0.12501562695336915\n",
      "epoch: 07100 | acc: 0.603 | loss: 0.802 | lr: 0.12347203358439313\n",
      "epoch: 07200 | acc: 0.607 | loss: 0.799 | lr: 0.12196609342602757\n",
      "epoch: 07300 | acc: 0.607 | loss: 0.796 | lr: 0.12049644535486204\n",
      "epoch: 07400 | acc: 0.613 | loss: 0.794 | lr: 0.11906179307060363\n",
      "epoch: 07500 | acc: 0.617 | loss: 0.791 | lr: 0.11766090128250381\n",
      "epoch: 07600 | acc: 0.620 | loss: 0.788 | lr: 0.11629259216187929\n",
      "epoch: 07700 | acc: 0.617 | loss: 0.785 | lr: 0.11495574203931487\n",
      "epoch: 07800 | acc: 0.620 | loss: 0.782 | lr: 0.11364927832708263\n",
      "epoch: 07900 | acc: 0.620 | loss: 0.779 | lr: 0.11237217664906168\n",
      "epoch: 08000 | acc: 0.627 | loss: 0.778 | lr: 0.11112345816201799\n",
      "epoch: 08100 | acc: 0.627 | loss: 0.775 | lr: 0.10990218705352237\n",
      "epoch: 08200 | acc: 0.627 | loss: 0.772 | lr: 0.10870746820306555\n",
      "epoch: 08300 | acc: 0.633 | loss: 0.769 | lr: 0.1075384449940854\n",
      "epoch: 08400 | acc: 0.633 | loss: 0.765 | lr: 0.10639429726566654\n",
      "epoch: 08500 | acc: 0.630 | loss: 0.762 | lr: 0.10527423939362038\n",
      "epoch: 08600 | acc: 0.630 | loss: 0.759 | lr: 0.10417751849150952\n",
      "epoch: 08700 | acc: 0.640 | loss: 0.755 | lr: 0.10310341272296113\n",
      "epoch: 08800 | acc: 0.647 | loss: 0.751 | lr: 0.1020512297173181\n",
      "epoch: 08900 | acc: 0.647 | loss: 0.747 | lr: 0.10102030508132134\n",
      "epoch: 09000 | acc: 0.647 | loss: 0.743 | lr: 0.1000100010001\n",
      "epoch: 09100 | acc: 0.647 | loss: 0.739 | lr: 0.09901970492127933\n",
      "epoch: 09200 | acc: 0.650 | loss: 0.736 | lr: 0.09804882831650162\n",
      "epoch: 09300 | acc: 0.650 | loss: 0.732 | lr: 0.09709680551509856\n",
      "epoch: 09400 | acc: 0.650 | loss: 0.729 | lr: 0.09616309260505818\n",
      "epoch: 09500 | acc: 0.647 | loss: 0.725 | lr: 0.09524716639679968\n",
      "epoch: 09600 | acc: 0.647 | loss: 0.722 | lr: 0.09434852344560807\n",
      "epoch: 09700 | acc: 0.657 | loss: 0.718 | lr: 0.09346667912889055\n",
      "epoch: 09800 | acc: 0.657 | loss: 0.715 | lr: 0.09260116677470137\n",
      "epoch: 09900 | acc: 0.657 | loss: 0.711 | lr: 0.09175153683824203\n",
      "epoch: 10000 | acc: 0.660 | loss: 0.708 | lr: 0.09091735612328393\n"
     ]
    }
   ],
   "source": [
    "# Create Dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create the network model. \n",
    "dense1      = Layer_Dense(2, 64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2      = Layer_Dense(64,3)\n",
    "\n",
    "# Create Softmax combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD(learning_rate=1.0, decay=1e-3, momentum=0.9)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Forward pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # Loss \n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "    \n",
    "    # Calculate accuracy. Calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2: \n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "\n",
    "    if not epoch % 100: \n",
    "        print(f\"epoch: {epoch:05d} | acc: {accuracy:.3f} | loss: {loss:.3f} | lr: {optimizer.current_learning_rate}\")\n",
    "\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.000\n",
      "1 0.909\n",
      "2 0.833\n",
      "3 0.769\n",
      "4 0.714\n",
      "5 0.667\n",
      "6 0.625\n",
      "7 0.588\n",
      "8 0.556\n",
      "9 0.526\n",
      "10 0.500\n",
      "11 0.476\n",
      "12 0.455\n",
      "13 0.435\n",
      "14 0.417\n",
      "15 0.400\n",
      "16 0.385\n",
      "17 0.370\n",
      "18 0.357\n",
      "19 0.345\n"
     ]
    }
   ],
   "source": [
    "# Test - decaying learning rate\n",
    "starting_learning_rate = 1.0\n",
    "learning_rate_decay    = 0.1\n",
    "\n",
    "for step in range(20):\n",
    "    learning_rate = starting_learning_rate * (1 /(1+learning_rate_decay*step))\n",
    "    print(step, f\"{learning_rate:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a8a3a8840d1b031f75a2a8404321b24da973faff270bc4dd0b1948a8f61d7454"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
