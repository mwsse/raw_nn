{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks from scratch in Python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases  = np.zeros((1, n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        # Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Gradient on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases  = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinputs  = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Since we need to modify the original variable, \n",
    "        # let's make a copy of the variable first \n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1,1)\n",
    "            # Calculate Jacobian matrix of the output ...\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "\n",
    "            # Calculate sample-wise gradient and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common loss class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "\n",
    "    # Calculates the data and regularization losses\n",
    "    # given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-entropy loss (subclass to Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    # Forwad pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "\n",
    "        # Probabilities for target values - \n",
    "        # only if categorical labels\n",
    "        # correct_confidences = 0  # Remove warning!\n",
    "\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "        \n",
    "        else:\n",
    "            print(\"ERROR!\")\n",
    "            correct_confidences = 0\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample.\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector \n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation_Softmax_Loss_CategoricalCrossEntropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "\n",
    "    # Creates activation and loss funtion objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss       = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        # If labels are one-hot encoded, turn them into discrete values\n",
    "        if len(y_true.shape) == 2: \n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "            \n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer SGD (Stochastic Gradient Descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "\n",
    "    def __init__(self, Learning_rate=1.0):\n",
    "        self.learning_rate = Learning_rate \n",
    "    \n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases  += -self.learning_rate * layer.dbiases\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 00000 | acc: 0.353 | loss: 1.099\n",
      "epoch: 00100 | acc: 0.460 | loss: 1.077\n",
      "epoch: 00200 | acc: 0.443 | loss: 1.066\n",
      "epoch: 00300 | acc: 0.443 | loss: 1.064\n",
      "epoch: 00400 | acc: 0.430 | loss: 1.063\n",
      "epoch: 00500 | acc: 0.423 | loss: 1.060\n",
      "epoch: 00600 | acc: 0.463 | loss: 1.056\n",
      "epoch: 00700 | acc: 0.470 | loss: 1.051\n",
      "epoch: 00800 | acc: 0.433 | loss: 1.056\n",
      "epoch: 00900 | acc: 0.443 | loss: 1.047\n",
      "epoch: 01000 | acc: 0.440 | loss: 1.043\n",
      "epoch: 01100 | acc: 0.433 | loss: 1.035\n",
      "epoch: 01200 | acc: 0.450 | loss: 1.024\n",
      "epoch: 01300 | acc: 0.443 | loss: 1.014\n",
      "epoch: 01400 | acc: 0.440 | loss: 1.006\n",
      "epoch: 01500 | acc: 0.437 | loss: 0.995\n",
      "epoch: 01600 | acc: 0.457 | loss: 0.989\n",
      "epoch: 01700 | acc: 0.470 | loss: 0.990\n",
      "epoch: 01800 | acc: 0.477 | loss: 0.991\n",
      "epoch: 01900 | acc: 0.457 | loss: 0.980\n",
      "epoch: 02000 | acc: 0.503 | loss: 0.965\n",
      "epoch: 02100 | acc: 0.443 | loss: 1.000\n",
      "epoch: 02200 | acc: 0.487 | loss: 0.955\n",
      "epoch: 02300 | acc: 0.440 | loss: 0.968\n",
      "epoch: 02400 | acc: 0.497 | loss: 0.941\n",
      "epoch: 02500 | acc: 0.470 | loss: 0.929\n",
      "epoch: 02600 | acc: 0.507 | loss: 0.962\n",
      "epoch: 02700 | acc: 0.527 | loss: 0.931\n",
      "epoch: 02800 | acc: 0.503 | loss: 0.880\n",
      "epoch: 02900 | acc: 0.493 | loss: 0.943\n",
      "epoch: 03000 | acc: 0.573 | loss: 0.853\n",
      "epoch: 03100 | acc: 0.550 | loss: 0.909\n",
      "epoch: 03200 | acc: 0.563 | loss: 0.877\n",
      "epoch: 03300 | acc: 0.463 | loss: 1.020\n",
      "epoch: 03400 | acc: 0.597 | loss: 0.868\n",
      "epoch: 03500 | acc: 0.577 | loss: 0.843\n",
      "epoch: 03600 | acc: 0.587 | loss: 0.866\n",
      "epoch: 03700 | acc: 0.610 | loss: 0.806\n",
      "epoch: 03800 | acc: 0.617 | loss: 0.800\n",
      "epoch: 03900 | acc: 0.560 | loss: 0.833\n",
      "epoch: 04000 | acc: 0.590 | loss: 0.876\n",
      "epoch: 04100 | acc: 0.597 | loss: 0.799\n",
      "epoch: 04200 | acc: 0.540 | loss: 0.835\n",
      "epoch: 04300 | acc: 0.533 | loss: 0.872\n",
      "epoch: 04400 | acc: 0.570 | loss: 0.828\n",
      "epoch: 04500 | acc: 0.583 | loss: 0.816\n",
      "epoch: 04600 | acc: 0.583 | loss: 0.783\n",
      "epoch: 04700 | acc: 0.517 | loss: 0.901\n",
      "epoch: 04800 | acc: 0.583 | loss: 0.812\n",
      "epoch: 04900 | acc: 0.590 | loss: 0.821\n",
      "epoch: 05000 | acc: 0.567 | loss: 0.845\n",
      "epoch: 05100 | acc: 0.573 | loss: 0.834\n",
      "epoch: 05200 | acc: 0.573 | loss: 0.824\n",
      "epoch: 05300 | acc: 0.580 | loss: 0.820\n",
      "epoch: 05400 | acc: 0.587 | loss: 0.818\n",
      "epoch: 05500 | acc: 0.593 | loss: 0.819\n",
      "epoch: 05600 | acc: 0.593 | loss: 0.824\n",
      "epoch: 05700 | acc: 0.593 | loss: 0.833\n",
      "epoch: 05800 | acc: 0.597 | loss: 0.832\n",
      "epoch: 05900 | acc: 0.600 | loss: 0.828\n",
      "epoch: 06000 | acc: 0.593 | loss: 0.821\n",
      "epoch: 06100 | acc: 0.590 | loss: 0.812\n",
      "epoch: 06200 | acc: 0.577 | loss: 0.806\n",
      "epoch: 06300 | acc: 0.580 | loss: 0.801\n",
      "epoch: 06400 | acc: 0.580 | loss: 0.799\n",
      "epoch: 06500 | acc: 0.577 | loss: 0.803\n",
      "epoch: 06600 | acc: 0.577 | loss: 0.822\n",
      "epoch: 06700 | acc: 0.547 | loss: 0.842\n",
      "epoch: 06800 | acc: 0.540 | loss: 0.870\n",
      "epoch: 06900 | acc: 0.530 | loss: 0.870\n",
      "epoch: 07000 | acc: 0.580 | loss: 0.858\n",
      "epoch: 07100 | acc: 0.617 | loss: 0.829\n",
      "epoch: 07200 | acc: 0.590 | loss: 0.806\n",
      "epoch: 07300 | acc: 0.603 | loss: 0.794\n",
      "epoch: 07400 | acc: 0.593 | loss: 0.788\n",
      "epoch: 07500 | acc: 0.607 | loss: 0.779\n",
      "epoch: 07600 | acc: 0.620 | loss: 0.753\n",
      "epoch: 07700 | acc: 0.623 | loss: 0.766\n",
      "epoch: 07800 | acc: 0.643 | loss: 0.745\n",
      "epoch: 07900 | acc: 0.690 | loss: 0.723\n",
      "epoch: 08000 | acc: 0.663 | loss: 0.720\n",
      "epoch: 08100 | acc: 0.637 | loss: 0.700\n",
      "epoch: 08200 | acc: 0.617 | loss: 0.718\n",
      "epoch: 08300 | acc: 0.653 | loss: 0.704\n",
      "epoch: 08400 | acc: 0.657 | loss: 0.686\n",
      "epoch: 08500 | acc: 0.643 | loss: 0.710\n",
      "epoch: 08600 | acc: 0.647 | loss: 0.702\n",
      "epoch: 08700 | acc: 0.640 | loss: 0.700\n",
      "epoch: 08800 | acc: 0.653 | loss: 0.698\n",
      "epoch: 08900 | acc: 0.683 | loss: 0.675\n",
      "epoch: 09000 | acc: 0.657 | loss: 0.690\n",
      "epoch: 09100 | acc: 0.663 | loss: 0.676\n",
      "epoch: 09200 | acc: 0.680 | loss: 0.691\n",
      "epoch: 09300 | acc: 0.693 | loss: 0.638\n",
      "epoch: 09400 | acc: 0.680 | loss: 0.714\n",
      "epoch: 09500 | acc: 0.693 | loss: 0.647\n",
      "epoch: 09600 | acc: 0.683 | loss: 0.666\n",
      "epoch: 09700 | acc: 0.693 | loss: 0.644\n",
      "epoch: 09800 | acc: 0.687 | loss: 0.661\n",
      "epoch: 09900 | acc: 0.713 | loss: 0.602\n",
      "epoch: 10000 | acc: 0.687 | loss: 0.627\n"
     ]
    }
   ],
   "source": [
    "# Create Dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "# Create the network model. \n",
    "dense1      = Layer_Dense(2, 64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2      = Layer_Dense(64,3)\n",
    "\n",
    "# Create Softmax combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD(Learning_rate=1)\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Forward pass\n",
    "    dense1.forward(X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # Loss \n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # Calculate accuracy. Calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2: \n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "\n",
    "    if not epoch % 100: \n",
    "        print(f\"epoch: {epoch:05d} | acc: {accuracy:.3f} | loss: {loss:.3f}\")\n",
    "\n",
    "    # Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a8a3a8840d1b031f75a2a8404321b24da973faff270bc4dd0b1948a8f61d7454"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
